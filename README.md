# ğŸš€ A/B Testing Simulator  
### Production-Grade Experimentation Framework for Product Data Science  

## ğŸ‘¤ Denis Agyapong  
**Product Data Scientist | Advanced Data Analyst**  
ğŸ“ Oakland, CA  
ğŸ“§ vantjohnn@gmail.com  
ğŸ”— [LinkedIn](https://linkedin.com/in/denis-agyapong)  
ğŸ™ [GitHub](https://github.com/Denis0242)

---

# ğŸ“Œ Executive Summary

A production-ready, end-to-end A/B testing experimentation framework designed for Product Data Science workflows.

This system combines:

- âœ… Frequentist hypothesis testing  
- âœ… Bayesian inference (PyMC)  
- âœ… Power analysis & sample size planning  
- âœ… Automated Go / No-Go recommendations  
- âœ… FastAPI production deployment  

It transforms experimentation from manual spreadsheet analysis into a reproducible, API-driven decision engine.

---

# ğŸ¯ Product Problem

Product teams frequently ask:

> â€œDid this feature actually improve the product?â€

However, experimentation often suffers from:

- Manual statistical calculations  
- Inconsistent methodology  
- Underpowered experiments  
- Misinterpretation of p-values  
- No standardized decision framework  

---

# ğŸ’¡ Solution

This simulator automates the complete experimentation lifecycle:

## 1ï¸âƒ£ Pre-Experiment
- Power analysis  
- Sample size calculation  
- Minimum Detectable Effect (MDE)  
- Achieved power validation  

## 2ï¸âƒ£ During Experiment
- Data validation  
- Outlier handling  
- Assumption checks  
- Balanced group verification  

## 3ï¸âƒ£ Post-Experiment
- Frequentist testing  
- Bayesian inference  
- Risk-adjusted decision logic  
- Final Go / No-Go recommendation  

---

# ğŸ§ª Real-World Use Case  
## Dark Mode Feature Launch

**Scenario:** Product team launches Dark Mode and evaluates impact.

**Primary Metric:** Session Duration (continuous)  
**Secondary Metric:** Conversion Rate (binary)

**Business Questions:**

- Does Dark Mode increase engagement?  
- Does it negatively impact conversion?  
- Should we roll it out globally?  

---

# ğŸ—ï¸ System Architecture

```
Data Input (CSV / API)
        â”‚
        â–¼
Data Loader & Validation
  - Outlier handling
  - Assumption checks
  - Descriptive statistics
        â”‚
        â–¼
Power Analysis
  - Sample size
  - Effect size
  - Achieved power
        â”‚
        â–¼
Frequentist Testing
  - T-test
  - Chi-square
  - Mann-Whitney U
  - Confidence intervals
        â”‚
        â–¼
Bayesian Inference (PyMC)
  - Posterior distributions
  - P(Variant > Control)
  - Expected loss
        â”‚
        â–¼
Decision Engine
  GO / CAUTION / NO-GO
        â”‚
        â–¼
FastAPI REST API
```

---

# ğŸ“Š Core Capabilities

## ğŸ”¹ Power Analysis & Experiment Design

- Detect 5% lift with 80% statistical power  
- Continuous and binary metric support  
- Cohenâ€™s d and Cohenâ€™s h effect sizes  
- Achieved power validation  

---

## ğŸ”¹ Frequentist Hypothesis Testing

Supports:

- Independent Samples T-Test  
- Chi-Square Test  
- Mann-Whitney U Test  
- 95% Confidence Intervals  
- Assumption checks (Shapiro-Wilk, Levene)  

---

## ğŸ”¹ Bayesian A/B Testing (PyMC)

Provides:

- Posterior probability: **P(Variant > Control)**  
- Highest Density Interval (HDI)  
- Expected loss quantification  
- More intuitive interpretation than p-values  

---

## ğŸ”¹ Automated Decision Framework

```
GO       â†’ Confidence â‰¥ 75%
CAUTION  â†’ 60% â‰¤ Confidence < 75%
NO-GO    â†’ Confidence < 60%
```

Decision integrates:
- Statistical significance  
- Practical effect size  
- Bayesian probability  
- Risk tolerance  

---

# ğŸš€ Quick Start

## Installation

```bash
git clone https://github.com/Denis0242/ab_testing_simulator.git
cd ab_testing_simulator

pip install -r requirements.txt
pip install pymc arviz
```

---

## Run Test Suite

```bash
python test_all.py
```

This validates:

- Data generation  
- Power calculations  
- Frequentist tests  
- Bayesian inference  
- End-to-end pipeline  

---

## Start FastAPI Server

```bash
python -m uvicorn app:app --reload
```

Access:

- API: http://localhost:8000  
- Swagger Docs: http://localhost:8000/docs  

---

# ğŸ“¡ API Endpoints

| Endpoint | Purpose |
|----------|----------|
| `/api/v1/analyze` | Full experiment analysis |
| `/api/v1/power-analysis` | Pre-experiment planning |
| `/api/v1/analyze-csv` | Upload CSV experiment |
| `/api/v1/sample-data` | Generate synthetic data |

---

# ğŸ“ Project Structure

```
ab_testing_simulator/
â”‚
â”œâ”€â”€ data_loader.py
â”œâ”€â”€ power_analysis.py
â”œâ”€â”€ hypothesis_testing.py
â”œâ”€â”€ bayesian_analysis.py
â”œâ”€â”€ analysis_pipeline.py
â”œâ”€â”€ app.py
â”œâ”€â”€ test_all.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ”¬ Statistical Foundations

## Frequentist

- Null hypothesis testing  
- P-values  
- Confidence intervals  
- Effect sizes  

## Bayesian

- Prior selection (Normal / Beta)  
- Posterior sampling (MCMC via PyMC)  
- Credible intervals (HDI)  
- Expected loss optimization  

---

# ğŸ“ˆ Example Result (Dark Mode Experiment)

Control:
- Mean Session Duration: 450.3s  
- Conversion Rate: 8.0%

Variant:
- Mean Session Duration: 480.2s  
- Conversion Rate: 8.5%

Results:

- T-test p-value = 0.001 â†’ Statistically significant  
- P(Variant > Control) = 98.5%  
- Confidence Score = 87.5%  

Final Recommendation: **GO**

---

# ğŸ§  Product Data Science Skills Demonstrated

- Experiment design  
- Power analysis  
- Statistical inference  
- Bayesian modeling  
- Decision science  
- API deployment (FastAPI)  
- Reproducible analytics pipelines  
- Production-ready testing framework  

---

# ğŸ’¡ Future Enhancements

- Sequential testing  
- Multi-armed bandits  
- CUPED variance reduction  
- Real-time streaming experiments  
- Uplift modeling  

---

# ğŸ“œ License

MIT License

---

# ğŸ¤ Contact

If you're a recruiter, hiring manager, or collaborator interested in Product Data Science experimentation systems, feel free to connect.

---

**Built for rigorous, scalable experimentation.**
